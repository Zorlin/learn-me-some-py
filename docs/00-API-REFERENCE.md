# LMSP API Reference\n\n**Complete API documentation with examples for all core LMSP modules.**\n\n## Table of Contents\n\n1. [Game State Management](#game-state-management)\n2. [Concept System](#concept-system)\n3. [Challenge System](#challenge-system)\n4. [Code Validation](#code-validation)\n5. [Adaptive Engine](#adaptive-engine)\n6. [Rendering](#rendering)\n7. [Input Systems](#input-systems)\n\n---\n\n## Game State Management\n\n### Module: `lmsp.game.state`\n\n#### GameState\n\nRepresents the complete state of a player's progress and current session.\n\n```python\nfrom lmsp.game.state import GameState, GameSession, Challenge\n\n# Create a new game state\nstate = GameState(\n    player_id=\"alice\",\n    current_level=1,\n    unlocked_concepts=[\"variables\", \"print\"],\n    completed_challenges={},\n    current_xp=0,\n)\n\n# Check if concept is unlocked\nif state.has_concept(\"lists\"):\n    print(\"Lists are unlocked!\")\nelse:\n    print(\"Need prerequisites first\")\n\n# Add XP and track progress\nstate.add_xp(50)\nprint(f\"Total XP: {state.current_xp}\")  # 50\n\n# Unlock a concept\nstate.unlock_concept(\"lists\")\n\n# Get player's statistics\nstats = state.get_statistics()\nprint(f\"Level: {stats['level']}\")\nprint(f\"Progress: {stats['progress_percent']}%\")\n```\n\n#### GameSession\n\nRepresents an active session with a specific challenge.\n\n```python\nfrom lmsp.game.state import GameSession, Challenge\n\n# Create a session\nsession = GameSession(\n    challenge=challenge_obj,\n    player_code=\"\",\n    attempts=0,\n    start_time=datetime.now(),\n)\n\n# Update player code\nsession.add_code(\"x = 5\")\nsession.add_code(\"print(x)\")\n\n# Check attempt count\nif session.attempts < 3:\n    session.increment_attempts()\nelse:\n    show_hint()  # Show hint after 3 attempts\n\n# Get session summary\nprint(f\"Attempts: {session.attempts}\")\nprint(f\"Code: {session.player_code}\")\n```\n\n---\n\n## Concept System\n\n### Module: `lmsp.python.concepts`\n\n#### ConceptLoader\n\nLoads and validates Python concept definitions from TOML files.\n\n```python\nfrom lmsp.python.concepts import ConceptLoader, ConceptDAG\n\n# Load all concepts\nloader = ConceptLoader(concepts_dir=\"concepts/\")\nconcepts = loader.load_all()\n\nprint(f\"Loaded {len(concepts)} concepts\")\n\n# Access a specific concept\nlist_concept = concepts[\"lists_basics\"]\nprint(f\"Name: {list_concept['name']}\")\nprint(f\"Level: {list_concept['level']}\")\nprint(f\"Prerequisites: {list_concept['prerequisites']}\")\nprint(f\"Unlocks: {list_concept['unlocks']}\")\n```\n\n#### ConceptDAG\n\nBuilds a directed acyclic graph of concepts with prerequisite tracking.\n\n```python\nfrom lmsp.python.concepts import ConceptDAG\n\n# Create the concept graph\ndag = ConceptDAG(concepts)\n\n# Check if prerequisites are met\nif dag.can_learn(\"list_comprehensions\", [\"lists\", \"for_loops\"]):\n    print(\"Ready to learn comprehensions!\")\n\n# Get all prerequisites\nprereqs = dag.get_prerequisites(\"classes\")\nprint(f\"Must learn first: {prereqs}\")\n\n# Get what unlocks after learning a concept\nunlocks = dag.get_unlocks(\"functions\")\nprint(f\"Now you can learn: {unlocks}\")\n\n# Get learning path to a goal\npath = dag.get_learning_path(\"async_await\")\nprint(\"Learning path:\")\nfor step in path:\n    print(f\"  - {step}\")\n\n# Detect cycles (should be none in valid DAG)\nhas_cycle = dag.has_cycle()\nprint(f\"DAG is valid: {not has_cycle}\")\n```\n\n---\n\n## Challenge System\n\n### Module: `lmsp.python.challenges`\n\n#### ChallengeLoader\n\nLoads challenge definitions from TOML files.\n\n```python\nfrom lmsp.python.challenges import ChallengeLoader\n\n# Load challenges\nloader = ChallengeLoader(challenges_dir=\"challenges/\")\nchallenges = loader.load_all()\n\nprint(f\"Loaded {len(challenges)} challenges\")\n\n# Get challenges by level\nlevel_2_challenges = [c for c in challenges if c.level == 2]\nprint(f\"Level 2 has {len(level_2_challenges)} challenges\")\n\n# Get challenges by concept\nlist_challenges = loader.get_by_concept(\"lists\")\nprint(f\"List challenges: {[c.name for c in list_challenges]}\")\n```\n\n#### Challenge\n\nRepresents a single coding challenge.\n\n```python\nfrom lmsp.python.challenges import Challenge\n\n# Access challenge details\nchallenge = challenges[0]\nprint(f\"Name: {challenge.name}\")\nprint(f\"Description: {challenge.description}\")\nprint(f\"Level: {challenge.level}\")\nprint(f\"Prerequisites: {challenge.prerequisites}\")\n\n# Get test cases\nfor test_case in challenge.test_cases:\n    print(f\"Input: {test_case.input}\")\n    print(f\"Expected: {test_case.expected}\")\n\n# Get hints by difficulty\nfor level, hint in enumerate(challenge.hints, 1):\n    print(f\"Hint {level}: {hint}\")\n\n# Get adaptive metadata\nprint(f\"Fun factor: {challenge.adaptive.fun_factor}\")\nprint(f\"Weakness signals: {challenge.adaptive.weakness_signals}\")\nprint(f\"Project themes: {challenge.adaptive.project_themes}\")\n```\n\n---\n\n## Code Validation\n\n### Module: `lmsp.python.validator`\n\n#### Validator\n\nValidates and executes player code in a sandboxed environment.\n\n```python\nfrom lmsp.python.validator import Validator, ValidationResult\n\n# Create validator\nvalidator = Validator(timeout=5.0, max_memory_mb=512)\n\n# Validate code against test cases\nplayer_code = \"\"\"\ndef add(a, b):\n    return a + b\n\"\"\"\n\ntest_cases = [\n    {\"input\": [1, 2], \"expected\": 3},\n    {\"input\": [5, 7], \"expected\": 12},\n]\n\nresult = validator.validate(player_code, test_cases, function_name=\"add\")\n\nif result.success:\n    print(\"✓ All tests passed!\")\nelse:\n    print(f\"✗ Test failed: {result.error}\")\n    print(f\"  Input: {result.failed_test.get('input')}\")\n    print(f\"  Expected: {result.failed_test.get('expected')}\")\n    print(f\"  Got: {result.actual_output}\")\n\n# Check for syntax errors\nif result.syntax_error:\n    print(f\"Syntax error: {result.syntax_error}\")\n\n# Check for runtime errors\nif result.runtime_error:\n    print(f\"Runtime error: {result.runtime_error}\")\n    print(f\"Traceback: {result.traceback}\")\n\n# Get execution metadata\nprint(f\"Execution time: {result.execution_time}ms\")\nprint(f\"Memory used: {result.memory_used}mb\")\n```\n\n#### Validation Result\n\n```python\nfrom lmsp.python.validator import ValidationResult\n\n# Result object contains:\nresult.success          # bool: Did all tests pass?\nresult.passed_tests     # int: How many tests passed?\nresult.total_tests      # int: Total number of tests\nresult.error            # str: Human-readable error message\nresult.syntax_error     # str: Python syntax error if any\nresult.runtime_error    # str: Runtime error if any\nresult.traceback        # str: Full Python traceback\nresult.actual_output    # Any: What the code actually returned\nresult.failed_test      # dict: Which test failed (if any)\nresult.execution_time   # float: How long code took (ms)\nresult.memory_used      # float: Memory used (MB)\n```\n\n---\n\n## Adaptive Engine\n\n### Module: `lmsp.adaptive.engine`\n\n#### AdaptiveEngine\n\nGenerates personalized curriculum based on learner profile.\n\n```python\nfrom lmsp.adaptive.engine import AdaptiveEngine, LearnerProfile\nfrom datetime import datetime\n\n# Create learner profile\nprofile = LearnerProfile(\n    player_id=\"alice\",\n    learning_style=\"visual\",  # visual, kinesthetic, auditory\n    fun_pattern=\"speedrun\",    # puzzle, speedrun, collection, etc.\n    created_at=datetime.now(),\n)\n\n# Create adaptive engine\nengine = AdaptiveEngine()\n\n# Get next recommended challenge\nnext_challenge = engine.recommend_challenge(\n    profile=profile,\n    available_challenges=all_challenges,\n    completed_challenges=player_completed,\n    current_level=player_level,\n)\n\nprint(f\"Recommended: {next_challenge.name}\")\nprint(f\"Reason: {next_challenge.reason}\")\n\n# Record completion\nengine.record_completion(\n    profile=profile,\n    challenge_id=\"lists_iteration\",\n    passed=True,\n    time_seconds=120,\n    attempts=2,\n)\n\n# Get personalized hints\nhints = engine.get_adaptive_hints(\n    profile=profile,\n    challenge=challenge,\n    attempt_count=3,\n)\nfor hint in hints:\n    print(f\"  - {hint}\")\n\n# Detect weaknesses\nweaknesses = engine.detect_weaknesses(profile)\nprint(f\"Areas to improve: {weaknesses}\")\n\n# Get spaced repetition schedule\nschedule = engine.get_review_schedule(profile)\nfor concept, next_review in schedule.items():\n    print(f\"Review '{concept}' on {next_review}\")\n```\n\n---\n\n## Rendering\n\n### Module: `lmsp.game.renderer`\n\n#### RichRenderer\n\nBeautiful terminal rendering using the Rich library.\n\n```python\nfrom lmsp.game.renderer import RichRenderer\nfrom lmsp.game.state import GameState\n\n# Create renderer\nrenderer = RichRenderer()\n\n# Render game state\nrenderer.render_state(game_state)\n\n# Render concept info\nrenderer.render_concept(concept_data)\n\n# Render challenge\nrenderer.render_challenge(challenge)\n\n# Render progress\nrenderer.render_progress(player_progress)\n\n# Render player code with syntax highlighting\nrenderer.render_code(\n    code=\"def hello():\\n    print('world')\",\n    language=\"python\"\n)\n\n# Render validation results\nrenderer.render_validation(\n    result=validation_result,\n    show_details=True,\n)\n```\n\n#### MinimalRenderer\n\nSimple text-based renderer for testing.\n\n```python\nfrom lmsp.game.renderer import MinimalRenderer\n\n# Create minimal renderer\nrenderer = MinimalRenderer()\n\n# Same API as RichRenderer, but plain text output\nrenderer.render_state(game_state)\nrenderer.render_challenge(challenge)\n```\n\n---\n\n## Input Systems\n\n### Module: `lmsp.input.emotional`\n\n#### EmotionalInput\n\nCaptures player engagement through analog feedback.\n\n```python\nfrom lmsp.input.emotional import EmotionalInput\n\n# Create emotional input handler\nemotional = EmotionalInput()\n\n# Log engagement event\nemotional.log_engagement(\n    challenge_id=\"lists_iteration\",\n    rt_value=0.8,  # RT trigger value (0-1)\n    lt_value=0.0,  # LT trigger value (0-1)\n    timestamp=datetime.now(),\n)\n\n# Get engagement profile\nengagement = emotional.get_engagement_profile(\n    challenge_id=\"lists_iteration\",\n    time_window_minutes=30,\n)\n\nprint(f\"Joy level: {engagement['joy']}\")\nprint(f\"Frustration level: {engagement['frustration']}\")\nprint(f\"Flow state: {engagement['flow']}\")\n\n# Get fun pattern\nfun_pattern = emotional.detect_fun_pattern(\n    challenge_id=\"lists_iteration\",\n    time_window_minutes=60,\n)\n\nprint(f\"Preferred fun type: {fun_pattern}\")\n```\n\n---\n\n## Example: Complete Challenge Flow\n\n```python\nfrom lmsp.game.state import GameState, GameSession\nfrom lmsp.python.challenges import ChallengeLoader\nfrom lmsp.python.validator import Validator\nfrom lmsp.adaptive.engine import AdaptiveEngine, LearnerProfile\nfrom lmsp.game.renderer import RichRenderer\nfrom datetime import datetime\n\n# Initialize systems\nstate = GameState(player_id=\"alice\", current_level=1)\nloader = ChallengeLoader(challenges_dir=\"challenges/\")\nvalidator = Validator()\nengine = AdaptiveEngine()\nrenderer = RichRenderer()\nprofile = LearnerProfile(player_id=\"alice\", learning_style=\"visual\")\n\n# Get recommended challenge\nchallenges = loader.load_all()\nchallenge = engine.recommend_challenge(\n    profile=profile,\n    available_challenges=challenges,\n    current_level=state.current_level,\n)\n\n# Render challenge to player\nrenderer.render_challenge(challenge)\n\n# Create session\nsession = GameSession(challenge=challenge)\n\n# Player writes code\nplayer_code = input(\"Enter your solution:\\n\")\nsession.add_code(player_code)\n\n# Validate code\nresult = validator.validate(\n    code=player_code,\n    test_cases=challenge.test_cases,\n)\n\n# Render results\nrenderer.render_validation(result)\n\nif result.success:\n    # Record completion\n    engine.record_completion(\n        profile=profile,\n        challenge_id=challenge.id,\n        passed=True,\n        time_seconds=120,\n        attempts=session.attempts,\n    )\n    \n    # Award XP and unlock next challenges\n    state.add_xp(challenge.xp_reward)\n    for unlocked in challenge.unlocks:\n        state.unlock_concept(unlocked)\n    \n    print(\"✓ Challenge complete!\")\nelse:\n    # Provide adaptive hints\n    hints = engine.get_adaptive_hints(profile, challenge, session.attempts)\n    for hint in hints:\n        print(f\"Hint: {hint}\")\n```\n\n---\n\n## Performance Characteristics\n\n### Concept DAG Operations\n- Loading concepts: O(n) where n = number of concepts\n- Topological sort: O(V + E) - linear time\n- Prerequisite check: O(V) worst case\n- Learning path calculation: O(V log V)\n\n### Code Validation\n- Parse time: Typically <1ms\n- Test case execution: Depends on code (usually <100ms)\n- Total validation: Usually <500ms for most challenges\n\n### Adaptive Engine\n- Recommendation: O(c) where c = candidate challenges\n- Weakness detection: O(h) where h = history size\n- Spaced repetition scheduling: O(1) per concept\n\n---\n\n## Error Handling\n\nAll LMSP modules use standard Python exceptions:\n\n```python\nfrom lmsp.python.concepts import ConceptDAG\nfrom lmsp.python.validator import ValidationError\n\ntry:\n    # Load and validate\n    dag = ConceptDAG(concepts)\n    path = dag.get_learning_path(\"nonexistent_concept\")\nexcept ValueError as e:\n    print(f\"Invalid concept: {e}\")\n\ntry:\n    # Validate code\n    result = validator.validate(code, test_cases, timeout=1.0)\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n```\n\n---\n\n## Common Patterns\n\n### Pattern 1: Adaptive Challenge Selection\n\n```python\n# Get next challenge based on learner profile\nchallenge = engine.recommend_challenge(\n    profile=learner_profile,\n    available=all_challenges,\n    exclude=completed_challenges,\n)\n```\n\n### Pattern 2: Code Validation with Hints\n\n```python\n# Validate and provide feedback\nresult = validator.validate(code, challenge.test_cases)\nif not result.success:\n    hints = engine.get_adaptive_hints(profile, challenge, attempt_count)\n```\n\n### Pattern 3: Progress Tracking\n\n```python\n# Track completion and update state\nengine.record_completion(profile, challenge_id, passed=True, time=120, attempts=2)\nstate.add_xp(challenge.xp_reward)\nstate.unlock_concept(challenge.unlocks[0])\n```\n\n---\n\n*For more detailed information, see the individual module documentation in `/docs/`*\n"