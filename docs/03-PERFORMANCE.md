# LMSP Performance Guide\n\n**Optimization strategies, profiling techniques, and performance characteristics.**\n\n## Performance Overview\n\n### Load Times\n\n| Operation | Time | Notes |\n|-----------|------|-------|\n| Startup | <1s | Load game engine |\n| Load all concepts | <100ms | 52 concept TOML files |\n| Build concept DAG | <50ms | Topological sort |\n| Load all challenges | <200ms | 60+ challenge files |\n| Start session | <10ms | Initialize game session |\n\n### Runtime Performance\n\n| Operation | Time | Complexity |\n|-----------|------|------------|\n| Code validation | 100-500ms | O(n) where n = lines of code |\n| Get recommendation | 50-200ms | O(c) where c = candidate challenges |\n| Render output | 10-50ms | O(t) where t = terminal width |\n| Prerequisite check | <1ms | O(1) with caching |\n| Get learning path | <10ms | O(V log V) |\n\n### Memory Usage\n\n| Component | Memory | Notes |\n|-----------|--------|-------|\n| Game state | <1MB | Player progress, current session |\n| All concepts | 2-5MB | 52 concepts in memory |\n| All challenges | 5-10MB | 60+ challenges in memory |\n| Renderer | <1MB | Terminal rendering buffers |\n| Validator instance | <5MB | Code execution sandbox |\n| **Total** | **15-25MB** | Typical runtime |\n\n## Concept & Challenge Loading\n\n### Current Performance\n\n```\n52 concepts loaded in <100ms\n60+ challenges loaded in <200ms\nConceptDAG built in <50ms\nTopological sort: O(V + E) = O(52 + edges)\n```\n\n### Optimization Strategies\n\n#### 1. Lazy Loading\n\nLoad concepts/challenges on demand:\n\n```python\nclass ConceptLoader:\n    def __init__(self, concepts_dir):\n        self._cache = {}  # Load on first access\n\n    def get(self, concept_id: str):\n        if concept_id not in self._cache:\n            self._cache[concept_id] = self._load_single(concept_id)\n        return self._cache[concept_id]\n```\n\n**Benefit**: 50ms startup instead of 100ms\n\n#### 2. Concept DAG Memoization\n\nCache expensive graph operations:\n\n```python\nclass ConceptDAG:\n    def __init__(self, concepts):\n        self._prereq_cache = {}  # Cache prerequisites\n        self._path_cache = {}    # Cache learning paths\n\n    def get_prerequisites(self, concept_id):\n        if concept_id not in self._prereq_cache:\n            self._prereq_cache[concept_id] = self._compute(concept_id)\n        return self._prereq_cache[concept_id]\n```\n\n**Benefit**: Prerequisite checks from 50ms to <1ms\n\n#### 3. Batch Challenge Loading\n\nLoad challenges by level for responsive selection:\n\n```python\nclass ChallengeLoader:\n    def __init__(self, challenges_dir):\n        # Load by level to support progressive disclosure\n        self.by_level = {}\n\n    def get_challenges_for_level(self, level):\n        if level not in self.by_level:\n            self.by_level[level] = self._load_level(level)\n        return self.by_level[level]\n```\n\n**Benefit**: Selection menu shows instantly\n\n### Profiling Code Loading\n\n```python\nimport cProfile\nimport pstats\n\nprof = cProfile.Profile()\nprof.enable()\n\n# Load concepts\nloader = ConceptLoader(\"concepts/\")\nconcepts = loader.load_all()\n\nprof.disable()\nstats = pstats.Stats(prof)\nstats.sort_stats('cumulative')\nstats.print_stats(10)  # Top 10 slowest\n```\n\nExpected output:\n```\nFunction                     ncalls  cumtime  perfunction\nload_all                          1    0.095    0.095\nparse_toml                       52    0.045    0.001\nvalidate_schema                  52    0.030    0.001\nbuild_graph                       1    0.015    0.015\ntopological_sort                  1    0.005    0.005\n```\n\n---\n\n## Code Validation Performance\n\n### Current Performance\n\n```\nAverage validation: 150ms\nMedian validation: 100ms\nMax validation (timeout): 5000ms\nMemory per execution: 10-20MB\n```\n\n### Optimization Strategies\n\n#### 1. Execution Caching\n\nCache results for identical code:\n\n```python\nclass Validator:\n    def __init__(self):\n        self._result_cache = {}  # code_hash -> result\n\n    def validate(self, code, test_cases):\n        code_hash = hash(code)\n        if code_hash in self._result_cache:\n            return self._result_cache[code_hash]\n\n        result = self._execute(code, test_cases)\n        self._result_cache[code_hash] = result\n        return result\n```\n\n**Benefit**: 100ms → <1ms for cached results\n\n#### 2. Timeout Optimization\n\nReduce timeout for simple challenges:\n\n```python\nclass Validator:\n    def validate(self, code, test_cases, timeout=None):\n        # Infer timeout from challenge complexity\n        if timeout is None:\n            line_count = len(code.split('\\n'))\n            timeout = max(1.0, min(5.0, line_count / 10))\n\n        # Use inferred timeout\n        result = self._execute_with_timeout(code, timeout)\n        return result\n```\n\n**Benefit**: Shorter waits for simple code\n\n#### 3. Memory-Constrained Sandbox\n\nLimit memory per execution:\n\n```python\nfrom resource import setrlimit, RLIMIT_AS\n\ndef sandbox_execute(code):\n    # Limit to 100MB\n    setrlimit(RLIMIT_AS, (100 * 1024 * 1024, 100 * 1024 * 1024))\n    exec(code)\n```\n\n**Benefit**: Prevent runaway allocations\n\n#### 4. Incremental Test Running\n\nStop on first failure:\n\n```python\nclass Validator:\n    def validate(self, code, test_cases, stop_on_first_fail=True):\n        for i, test in enumerate(test_cases):\n            result = self._run_test(code, test)\n            if not result.success and stop_on_first_fail:\n                return ValidationResult(\n                    success=False,\n                    passed_tests=i,\n                    failed_test=test,\n                    # ... other fields\n                )\n        return result_success\n```\n\n**Benefit**: Validation from 500ms → 100ms for failing code\n\n### Profiling Validation\n\n```python\nimport timeit\n\n# Measure validation time\ncode = '''\nx = [1, 2, 3, 4, 5]\nprint(sum(x))\n'''\n\nvalidator = Validator()\ntime_ms = timeit.timeit(\n    lambda: validator.validate(code, [{\"expected\": 15}]),\n    number=1\n) * 1000\n\nprint(f\"Validation: {time_ms:.0f}ms\")\n```\n\n---\n\n## Adaptive Engine Performance\n\n### Current Performance\n\n```\nRecommendation: 50-200ms\nWeakness detection: 10-50ms\nSchedule calculation: <1ms per concept\nFull session analysis: 100-500ms\n```\n\n### Optimization Strategies\n\n#### 1. Batch Processing\n\nProcess multiple events together:\n\n```python\nclass AdaptiveEngine:\n    def __init__(self):\n        self._pending_events = []\n        self._batch_size = 10\n\n    def record_attempt(self, **kwargs):\n        self._pending_events.append(kwargs)\n        if len(self._pending_events) >= self._batch_size:\n            self._process_batch()\n\n    def _process_batch(self):\n        # Process all at once\n        for event in self._pending_events:\n            self._update_profile(event)\n        self._pending_events = []\n```\n\n**Benefit**: From 10ms × N → 100ms per batch of N\n\n#### 2. Lazy Spaced Repetition\n\nOnly calculate schedules on demand:\n\n```python\nclass AdaptiveEngine:\n    def __init__(self):\n        self._schedule_cache = {}  # concept_id -> next_review\n\n    def get_review_schedule(self, concept_id):\n        if concept_id not in self._schedule_cache:\n            self._schedule_cache[concept_id] = self._calculate(concept_id)\n        return self._schedule_cache[concept_id]\n\n    def invalidate_schedule(self, concept_id):\n        # Clear cache when concept is practiced\n        if concept_id in self._schedule_cache:\n            del self._schedule_cache[concept_id]\n```\n\n**Benefit**: Schedule queries from 50ms → <1ms\n\n#### 3. Incremental Recommendation\n\nUse weighted random sampling:\n\n```python\nclass AdaptiveEngine:\n    def recommend_next(self, num_candidates=5):\n        # Don't score all challenges, sample weighted by fit\n        candidates = random.choices(\n            all_challenges,\n            weights=self._compute_weights(all_challenges),\n            k=num_candidates\n        )\n\n        # Score only candidates\n        best = max(candidates, key=lambda c: self._score(c))\n        return best\n```\n\n**Benefit**: Recommendation from 200ms → 50ms\n\n---\n\n## Rendering Performance\n\n### Current Performance\n\n```\nRich rendering: 10-50ms\nMinimal rendering: 1-5ms\nTerminal output: 10-100ms (depends on terminal)\nFull screen redraw: <100ms\n```\n\n### Optimization Strategies\n\n#### 1. Incremental Rendering\n\nOnly update changed areas:\n\n```python\nclass RichRenderer:\n    def __init__(self):\n        self._last_state = None\n\n    def render_state(self, state):\n        if state == self._last_state:\n            return  # No change, skip render\n\n        # Only render changed sections\n        if state.code != self._last_state.code:\n            self._render_code(state.code)\n        if state.tests_passing != self._last_state.tests_passing:\n            self._render_progress(state.tests_passing)\n\n        self._last_state = state\n```\n\n**Benefit**: Rendering from 50ms → 5ms for unchanged content\n\n#### 2. Buffer Pooling\n\nReuse string buffers:\n\n```python\nclass RichRenderer:\n    def __init__(self):\n        self._buffer_pool = [io.StringIO() for _ in range(5)]\n\n    def render(self, content):\n        buf = self._buffer_pool.pop()\n        try:\n            buf.write(content)\n            self.console.print(buf.getvalue())\n        finally:\n            buf.truncate(0)\n            buf.seek(0)\n            self._buffer_pool.append(buf)\n```\n\n**Benefit**: Reduce string allocation overhead\n\n#### 3. Lazy Syntax Highlighting\n\nOnly highlight when showing code:\n\n```python\nclass RichRenderer:\n    def render_code(self, code, show_syntax_highlight=False):\n        if show_syntax_highlight:\n            highlighted = self._highlight_syntax(code)\n            self.console.print(highlighted)\n        else:\n            # Plain rendering for long code\n            self.console.print(code)\n```\n\n**Benefit**: Large code files render faster\n\n---\n\n## Memory Optimization\n\n### Monitor Memory Usage\n\n```python\nimport tracemalloc\n\ntracemalloc.start()\n\n# Run game\nengine = GameEngine(profile)\nengine.run()\n\ncurrent, peak = tracemalloc.get_traced_memory()\nprint(f\"Current: {current / 1024 / 1024:.1f} MB\")\nprint(f\"Peak: {peak / 1024 / 1024:.1f} MB\")\n\n# Get top allocators\nsnapshot = tracemalloc.take_snapshot()\ntop_stats = snapshot.statistics('lineno')\nfor stat in top_stats[:10]:\n    print(stat)\n```\n\n### Optimization Strategies\n\n#### 1. Cache Eviction\n\nLimit cache size:\n\n```python\nfrom functools import lru_cache\n\nclass ConceptDAG:\n    @lru_cache(maxsize=128)\n    def get_prerequisites(self, concept_id):\n        # Cache up to 128 results\n        return self._compute(concept_id)\n```\n\n#### 2. Generator-Based Loading\n\nStream challenges instead of loading all:\n\n```python\ndef load_challenges_lazy(challenges_dir):\n    for toml_file in challenges_dir.glob(\"*.toml\"):\n        challenge = load_challenge(toml_file)\n        yield challenge\n\n# Use as:\nfor challenge in load_challenges_lazy(\"challenges/\"):\n    if challenge.level == 2:\n        process(challenge)\n```\n\n#### 3. String Interning\n\nReuse strings:\n\n```python\n# Instead of multiple copies:\nstatement1 = \"lists_basics\"\nstatement2 = \"lists_basics\"\n\n# Intern to single string:\nstatement1 = sys.intern(\"lists_basics\")\nstatement2 = sys.intern(\"lists_basics\")\nassert statement1 is statement2  # Same object\n```\n\n---\n\n## Scaling Considerations\n\n### Current Limits\n\n| Metric | Current | Scaling |\n|--------|---------|----------|\n| Concepts | 52 | Linear up to 500 |\n| Challenges | 60+ | Linear up to 1000 |\n| Player sessions | Single | 10-100 concurrent |\n| Concept DAG size | 52 nodes | O(V+E) stays fast <1000 |\n\n### Future Optimizations (Phase 7+)\n\n1. **Database Backend**\n   - Move from TOML files to SQLite\n   - Indexed queries\n   - Faster concept lookups\n\n2. **Distributed Cache**\n   - Redis for shared recommendations\n   - Faster multi-player sessions\n\n3. **Async Rendering**\n   - Non-blocking validation\n   - Responsive UI\n\n4. **Compression**\n   - Gzip challenge data\n   - Reduce memory footprint\n\n---\n\n## Benchmarking\n\n### Running Benchmarks\n\n```bash\n# Benchmark concept loading\npython -m timeit -s \"from lmsp.python.concepts import ConceptLoader\" \\\n  \"ConceptLoader('concepts/').load_all()\"\n\n# Benchmark validation\npython -m timeit -s \"from lmsp.python.validator import Validator; \\\nv=Validator(); code='x=1'\" \\\n  \"v.validate(code, [{'expected': None}])\"\n\n# Benchmark recommendation\npython -m timeit -s \"from lmsp.adaptive.engine import AdaptiveEngine, LearnerProfile; \\\ne=AdaptiveEngine(); p=LearnerProfile('test')\" \\\n  \"e.recommend_next(p)\"\n```\n\n### Expected Results\n\n```\nConceptLoader.load_all(): 100ms\nValidator.validate(): 150ms (first time), 1ms (cached)\nAdaptiveEngine.recommend_next(): 80ms\n```\n\n---\n\n## Monitoring in Production\n\n### Log Performance\n\n```python\nimport logging\nimport time\n\nlogger = logging.getLogger(__name__)\n\ndef timed_operation(name, operation):\n    start = time.time()\n    result = operation()\n    elapsed = time.time() - start\n    logger.info(f\"{name}: {elapsed*1000:.1f}ms\")\n    return result\n\n# Use:\nresult = timed_operation(\n    \"Load concepts\",\n    lambda: ConceptLoader(\"concepts/\").load_all()\n)\n```\n\n### Alert on Slowness\n\n```python\ndef slow_operation_warning(name, operation, threshold_ms=100):\n    start = time.time()\n    result = operation()\n    elapsed = (time.time() - start) * 1000\n\n    if elapsed > threshold_ms:\n        logger.warning(\n            f\"SLOW: {name} took {elapsed:.0f}ms (threshold: {threshold_ms}ms)\"\n        )\n\n    return result\n```\n\n---\n\n## Checklist for Performance\n\nWhen optimizing, verify:\n\n- [ ] Baseline measured before optimization\n- [ ] Profiler shows clear bottleneck\n- [ ] Change isolated to one module\n- [ ] After-change measurement taken\n- [ ] Improvement >= 20% before shipping\n- [ ] No memory increase\n- [ ] Tests still pass\n- [ ] Real-world usage tested (not just micro-benchmarks)\n\n---\n\n*See also: Profiling tools (cProfile, tracemalloc, line_profiler), System monitoring (top, htop)*\n"