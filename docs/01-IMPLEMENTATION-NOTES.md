# LMSP Implementation Notes\n\n**Technical implementation details, design decisions, and architectural patterns used in LMSP Phase 1.**\n\n## Core Modules\n\n### 1. Game State Management (`lmsp/game/state.py`)\n\n#### Design Pattern: Immutable State + Functional Updates\n\nThe `GameState` class follows a functional programming pattern where:\n- State is represented as plain dataclass\n- Updates create new state objects (or modify in place, careful with references)\n- Sessions are atomic units of play\n\n```python\n# State is a snapshot\nstate = GameState(player_id=\"alice\", current_level=1)\n\n# Modifications\nstate.add_xp(50)  # Modifies in place (could be made immutable)\nstate.unlock_concept(\"lists\")  # Updates internal state\n```\n\n#### Key Implementation Details\n\n1. **XP and Leveling**: Linear progression based on total XP\n   - Level calculation: `level = total_xp // xp_per_level + 1`\n   - This could be made more sophisticated with polynomial growth\n\n2. **Concept Tracking**: Maintains two sets\n   - `unlocked_concepts`: Can attempt challenges\n   - `completed_challenges`: Mastered (for spaced repetition)\n\n3. **Session Checkpointing**: GameSession stores intermediate state\n   - Allows rollback if player wants to redo\n   - TAS (tool-assisted speedrun) relies on this\n\n#### Potential Improvements\n\n- **Make state truly immutable**: Use frozen dataclasses or implement with `@property`\n- **Add state versioning**: Track history for debugging\n- **Implement proper serialization**: JSON/pickle support for persistence\n- **Add async checkpoint system**: Non-blocking saves to disk\n\n---\n\n### 2. Concept System (`lmsp/python/concepts.py`)\n\n#### Design Pattern: Graph-Based Learning Model\n\nConcepts form a DAG (Directed Acyclic Graph) where:\n- Nodes = Python concepts\n- Edges = prerequisite relationships\n- Paths = valid learning sequences\n\n#### Implementation Architecture\n\n```\nConcept TOML Files\n       ↓\nConceptLoader (parses & validates)\n       ↓\nConceptDAG (builds graph)\n       ↓\nTopological Sort (valid learning order)\n```\n\n#### Key Algorithms\n\n1. **Topological Sort** (Kahn's Algorithm): O(V + E)\n   ```python\n   # Order concepts so prerequisites come first\n   sorted_concepts = dag.topological_sort()\n   ```\n\n2. **Prerequisite Checking**: O(V) worst case\n   ```python\n   # BFS to find all prerequisites\n   def get_prerequisites(concept_id):\n       # Returns all concepts needed before this one\n   ```\n\n3. **Learning Path Finding**: O(V log V)\n   ```python\n   # Dijkstra-like algorithm to find optimal learning sequence\n   path = dag.get_learning_path(goal_concept)\n   ```\n\n#### TOML Schema\n\nEach concept file has:\n- `[concept]` section: metadata (id, level, prerequisites, unlocks)\n- `[description]` section: learning content\n- `[examples]` section: multiple code examples\n- `[adaptive]` section: fun patterns, weakness signals\n\nExample:\n```toml\n[concept]\nid = \"lists_basics\"\nname = \"Introduction to Lists\"\nlevel = 2\nprerequisites = [\"variables\", \"types\"]\nunlocks = [\"list_comprehensions\", \"sorting\"]\n```\n\n#### Cycle Detection\n\nDue to `networkx` integration, cycles are automatically detected:\n```python\nif dag.has_cycle():\n    raise ValueError(\"Concept graph contains cycles!\")\n```\n\nThis prevents malformed curriculum definitions.\n\n---\n\n### 3. Challenge System (`lmsp/python/challenges.py`)\n\n#### Challenge Definition Format\n\nEach challenge is a TOML file with:\n\n1. **Metadata** section\n   - `id`: Unique identifier\n   - `name`: Display name\n   - `level`: Difficulty level (0-6)\n   - `prerequisites`: Required concepts\n   - `unlocks`: What it enables\n\n2. **Tests** section\n   - Input/output pairs\n   - Can test functions, print output, or complex behavior\n\n3. **Adaptive** section\n   - `fun_factor`: Type of fun (puzzle, speedrun, collection, creation, competition, mastery)\n   - `weakness_signals`: What skill gaps this reveals\n   - `project_themes`: Real-world use cases\n\n#### Test Case Execution\n\nTest cases are validated using the `Validator` class:\n```python\ntest_case = {\n    \"input\": [1, 2],\n    \"expected\": 3,\n}\nresult = validator.validate(player_code, [test_case])\n```\n\nThe validator:\n1. Parses player code (syntax check)\n2. Executes in sandboxed environment\n3. Captures output\n4. Compares against expected\n5. Returns detailed error if mismatch\n\n#### Metadata Extraction\n\nThe `ChallengeLoader` extracts:\n- All prerequisites (computed transitively)\n- Prerequisites' prerequisites (builds complete tree)\n- Compatible concepts for adaptive learning\n\n---\n\n### 4. Code Validation (`lmsp/python/validator.py`)\n\n#### Sandboxed Execution Design\n\nThe validator executes untrusted player code safely:\n\n```python\nvalidator = Validator(\n    timeout=5.0,           # Kill after 5 seconds\n    max_memory_mb=512,     # Limit memory usage\n    max_output_kb=1024,    # Limit stdout/stderr\n)\n\nresult = validator.validate(\n    code=player_code,\n    test_cases=tests,\n    function_name=\"solve\",  # Function to test\n)\n```\n\n#### Security Features\n\n1. **Timeout Protection**: `signal.alarm()` or `asyncio` timeouts\n   - Prevents infinite loops\n   - Kills runaway processes\n\n2. **Memory Limits**: `resource` module (Unix/Linux)\n   - Prevents memory exhaustion\n   - Raises MemoryError if exceeded\n\n3. **Restricted Builtins**: Can whitelist safe functions\n   - Prevents `__import__()`, `eval()`, etc.\n   - Maintains learner safety\n\n4. **Output Capture**: Redirects stdout/stderr\n   - Captures print statements\n   - Collects error messages\n\n#### Execution Flow\n\n```\nPlayer Code\n    ↓\n[1] Parse (AST)\n    ↓\n[2] Syntax Check\n    ↓\n[3] Setup Sandbox (import restrictions, timeout)\n    ↓\n[4] Execute Code\n    ↓\n[5] Capture Output\n    ↓\n[6] Run Test Cases\n    ↓\n[7] Compare Results\n    ↓\nValidationResult\n```\n\n#### Result Object\n\nThe `ValidationResult` provides:\n- `success`: Boolean - all tests passed?\n- `passed_tests`: Count of passing tests\n- `error`: Human-friendly error message\n- `syntax_error`: Python syntax error (if any)\n- `runtime_error`: Execution error (if any)\n- `traceback`: Full Python traceback\n- `actual_output`: What the code returned\n- `failed_test`: Which test failed\n- `execution_time`: Milliseconds to execute\n- `memory_used`: Peak memory in MB\n\n---\n\n### 5. Adaptive Engine (`lmsp/adaptive/engine.py`)\n\n#### Recommendation System\n\nThe engine recommends next challenges based on:\n\n1. **Learner Profile**\n   - Learning style (visual, kinesthetic, auditory)\n   - Fun pattern preferences\n   - Current mastery level\n\n2. **Challenge Scoring**\n   - Difficulty vs current level (sweet spot: slightly challenging)\n   - Prerequisites met?\n   - Fun factor matches preferences?\n   - Time since last review (spaced repetition)\n\n3. **Historical Performance**\n   - Completion history\n   - Time spent on similar concepts\n   - Success rate on this concept type\n   - Weakness patterns\n\n#### Spaced Repetition Foundation\n\nThe engine can implement Anki's SM-2 algorithm:\n\n```python\n# Track concept mastery\nmastery[concept] = {\n    \"interval\": 1,      # Days until next review\n    \"ease_factor\": 2.5, # Difficulty multiplier\n    \"repetitions\": 0,   # How many times studied\n}\n\n# Update based on performance\nif performance == \"easy\":\n    mastery[\"ease_factor\"] += 0.1\n    mastery[\"interval\"] *= mastery[\"ease_factor\"]\nelif performance == \"hard\":\n    mastery[\"ease_factor\"] = max(1.3, mastery[\"ease_factor\"] - 0.2)\n    mastery[\"interval\"] = 1\n```\n\n#### Fun Pattern Detection\n\nSix types of fun:\n1. **Puzzle**: Solving problems (logic, pattern matching)\n2. **Speedrun**: Time pressure (fast execution, optimization)\n3. **Collection**: Completing sets (achievements, milestones)\n4. **Creation**: Building things (projects, expression)\n5. **Competition**: Against others (racing, high scores)\n6. **Mastery**: Getting very good (flow state, difficulty scaling)\n\nThe engine tracks emotional input (RT/LT triggers) to detect which fun the player experiences.\n\n---\n\n### 6. Rendering (`lmsp/game/renderer.py`)\n\n#### Two Renderer Implementations\n\n1. **RichRenderer**: Beautiful terminal UI using the `rich` library\n   - Colors, tables, panels, progress bars\n   - Syntax highlighting for code\n   - Responsive layout\n\n2. **MinimalRenderer**: Plain text output\n   - No dependencies\n   - Perfect for testing\n   - Easy to debug\n\n#### Rendering Strategy\n\nBoth implement the same interface:\n```python\nrenderer.render_state(game_state)      # Main game view\nrenderer.render_challenge(challenge)   # Challenge description\nrenderer.render_code(code)             # Syntax highlighted code\nrenderer.render_validation(result)     # Test results\nrenderer.render_progress(stats)        # Player stats\n```\n\nThis allows:\n- Swapping renderers at runtime\n- Testing without terminal dependencies\n- Different rendering strategies\n\n#### Rich Library Features\n\nThe RichRenderer leverages:\n- **Syntax highlighting**: Python code with colors\n- **Tables**: Display test results in grid format\n- **Panels**: Group related content\n- **Progress bars**: Visual progress indication\n- **Markdown**: Render formatted text\n- **Colors**: Theme-aware color schemes\n\n---\n\n### 7. Emotional Input (`lmsp/input/emotional.py`)\n\n#### Analog Engagement Tracking\n\nThe emotional input system captures:\n- **RT (Right Trigger)**: Positive engagement (0.0-1.0)\n- **LT (Left Trigger)**: Negative engagement (0.0-1.0)\n- **Timestamp**: When the engagement occurred\n\nThis provides analog data about player emotional state during challenges.\n\n#### Fun Pattern Recognition\n\nBy analyzing trigger patterns over time:\n- RT peaks during certain challenge types → player enjoys puzzles\n- LT spikes on specific concepts → player finds them frustrating\n- Sustained RT during projects → player likes creation\n\n#### Integration with Adaptive Engine\n\nThe engine uses emotional data to:\n1. Predict what challenges player will enjoy\n2. Detect struggle (high LT) and offer help\n3. Recognize flow state (moderate RT sustained)\n4. Adjust difficulty dynamically\n\n---\n\n## Architecture Patterns\n\n### 1. Dependency Injection\n\nModules accept dependencies:\n```python\ndef __init__(self, renderer=None, validator=None):\n    self.renderer = renderer or RichRenderer()\n    self.validator = validator or Validator()\n```\n\nBenefits:\n- Testable (inject test doubles)\n- Flexible (swap implementations)\n- Decoupled (modules don't know about concrete implementations)\n\n### 2. Builder Pattern\n\nComplex objects use builders:\n```python\nchallenge = ChallengeBuilder()\n    .with_name(\"Lists 101\")\n    .with_level(2)\n    .with_test_case([1, 2], 3)\n    .build()\n```\n\n### 3. Strategy Pattern\n\nDifferent rendering strategies:\n```python\nif use_rich:\n    renderer = RichRenderer()\nelse:\n    renderer = MinimalRenderer()\n\nrenderer.render(content)  # Same interface, different output\n```\n\n### 4. Observer Pattern (for future)\n\nEmotional input notifies adaptive engine:\n```python\nemotional.subscribe(on_engagement=engine.handle_engagement)\nemotional.log_event(rt=0.8)  # Triggers engine update\n```\n\n## Performance Optimizations\n\n### 1. Concept DAG Caching\n\n```python\nclass ConceptDAG:\n    def __init__(self, concepts):\n        self._cache = {}  # Memoization\n        self.graph = nx.DiGraph()\n```\n\nCompute expensive paths once, reuse results.\n\n### 2. Test Case Batching\n\nValidate multiple test cases in single execution:\n```python\nresult = validator.validate(code, multiple_test_cases)\n```\n\nMore efficient than running code multiple times.\n\n### 3. Lazy Loading\n\nConcepts and challenges loaded on demand:\n```python\nloader = ConceptLoader()\nconcepts = {}  # Empty\n\n# Load when accessed\nif concept_id not in concepts:\n    concepts[concept_id] = loader.load_single(concept_id)\n```\n\n## Testing Strategy\n\n### Unit Tests\n\nEach module has comprehensive unit tests:\n- `tests/test_concepts.py` - ConceptDAG, topological sort, cycles\n- `tests/test_challenges.py` - Challenge loading, metadata\n- `tests/test_validator.py` - Code execution, sandboxing\n- `tests/test_renderer.py` - Rendering output\n- `tests/test_state.py` - State management\n\n### Test Coverage\n\n```bash\npytest tests/ --cov=lmsp --cov-report=html\n```\n\nTarget: 80%+ coverage for all modules\n\n### Mocking Strategy\n\nUse `unittest.mock` for dependencies:\n```python\nfrom unittest.mock import Mock, patch\n\nengine = AdaptiveEngine()\nengine.validator = Mock()  # Inject test double\nengine.renderer = Mock()   # Inject test double\n```\n\n## Future Improvements\n\n### Phase 2: Gamepad Input\n- Pygame integration for gamepad detection\n- Radial thumbstick typing (8-direction chords)\n- Easy Mode button mapping\n- Haptic feedback\n\n### Phase 3: Adaptive Systems\n- SM-2 spaced repetition algorithm\n- Fun pattern tracker\n- Weakness detection and resurfacing\n- Project-driven curriculum generator\n\n### Phase 4: Multiplayer\n- Claude AI players via API\n- Session sync for shared state\n- Split-screen UI\n- Teaching mode with explanations\n\n### Phase 5: Introspection\n- Screenshot capture with AST metadata\n- Strategic video recording (mosaic tiles)\n- TAS (tool-assisted speedrun) features\n- Checkpoint/restore system\n\n### Phase 6: Polish\n- Visual themes\n- Achievement system\n- Progress visualization\n- Audio feedback\n- Community content support\n\n---\n\n## Common Pitfalls & Solutions\n\n### Pitfall 1: Blocking on Code Execution\n\n**Problem**: Long-running player code freezes the UI\n\n**Solution**: Implement timeout with `signal.alarm()` or `asyncio`\n\n```python\nimport signal\n\ndef timeout_handler(signum, frame):\n    raise TimeoutError(\"Code execution timed out\")\n\nsignal.signal(signal.SIGALRM, timeout_handler)\nsignal.alarm(5)  # 5 second timeout\ntry:\n    exec(player_code)\nfinally:\n    signal.alarm(0)  # Cancel alarm\n```\n\n### Pitfall 2: Memory Leaks in Validator\n\n**Problem**: Each execution creates new namespace, references not cleared\n\n**Solution**: Clear namespace after execution\n\n```python\nnamespace = {}\nexec(code, namespace)\n\n# Clear everything except return value\nresult = namespace.get('result')\nnamespace.clear()\ndel namespace\n```\n\n### Pitfall 3: State Mutations\n\n**Problem**: GameState modifications affect other sessions\n\n**Solution**: Use immutable patterns or deep copy\n\n```python\nimport copy\n\n# Make a copy to avoid mutations\nstate_copy = copy.deepcopy(original_state)\nstate_copy.add_xp(50)  # Only affects copy\n```\n\n### Pitfall 4: Circular Imports\n\n**Problem**: `module_a` imports `module_b` imports `module_a`\n\n**Solution**: Move shared types to `types.py` or use TYPE_CHECKING\n\n```python\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from lmsp.game.state import GameState\n```\n\n---\n\n## Debugging Tips\n\n### Enable Verbose Logging\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\nlogger.debug(f\"Loading challenge: {challenge_id}\")\n```\n\n### Use MinimalRenderer for Testing\n\n```python\n# In tests, use plain text renderer\nrenderer = MinimalRenderer()\n\n# Output is easy to assert on\noutput = renderer.render_state(state)\nassert \"Level 1\" in output\n```\n\n### Inspect GameState\n\n```python\nprint(f\"XP: {state.current_xp}\")\nprint(f\"Level: {state.get_statistics()['level']}\")\nprint(f\"Unlocked: {state.unlocked_concepts}\")\n```\n\n### Validate ConceptDAG\n\n```python\nif dag.has_cycle():\n    print(\"ERROR: Concept graph has cycles!\")\nelse:\n    print(f\"Valid DAG with {len(dag)} concepts\")\n```\n\n---\n\n*See individual module files for detailed code comments and examples.*\n"